{"cells":[{"source":"# Data4Good Case Challenge\n\n\n\n![Data4Good](Data4Good.png)\n\n\n\n## üìñ Background\nArtificial Intelligence (AI) is rapidly transforming education by providing students with instant access to information and adaptive learning tools. Still, it also introduces significant risks, such as the spread of misinformation and fabricated content. Research indicates that large language models (LLMs) often confidently generate factually incorrect or ‚Äúhallucinated‚Äù responses, which can mislead learners and erode trust in digital learning platforms. \n\nThe 4th Annual Data4Good Competition challenges participants to develop innovative analytics solutions to detect and improve factuality in AI-generated educational content, ensuring that AI advances knowledge rather than confusion.","metadata":{},"id":"b1684a8d-c5a7-4596-bfe4-b37c611971d7","cell_type":"markdown"},{"source":"## üíæ The data\n\nThe data provided is a Questions/Answer dataset to determine if the answer is factual, not factual (contradiction), or irrelevant to the question.\n\n\n- Question: The question asked/prompted for\n- Context: Relevant contextual support for the question\n- Answer: The answer provided by an AI\n- Type:  A categorical variable with three possible levels ‚Äì Factual, Contradiction, Irrelevant:\n  - Factual: the answer is correct\n  - Contradiction: the answer is incorrect\n  - Irrelevant: the answer has nothing to do with the question\n  \nThere are 21,021 examples in the dataset (`data/train.json`) that you will experiment with. \n\n\nThe test dataset (`data/test.json`) contains 2000 examples that you predict as one of the three provided classes. In addition to classification performance we are seeking as detailed as possible methodologies of your step-by-step approach in your notebooks. Discuss what worked well, what did not work well, and your suggestions or ideas if a general approach to these types of problems might exist.\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"7799095a-fd55-455b-b3d2-8c9c7371586c","cell_type":"markdown"},{"source":"### Previewing the Training Data\n\nLet's load and preview the `train.json` dataset to understand its structure and contents.","metadata":{},"id":"f1a5f9b7-9c8a-4a7f-a738-d6212881cd46","cell_type":"markdown"},{"source":"import os, json, re\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom scipy.sparse import csr_matrix\nfrom sklearn.svm import LinearSVC\n\n# =========================\n# 0) Locate data files\n# =========================\nCANDIDATE_DATA_DIRS = [\n    \"data\",\n    \"/shared-templates/competition-python-data4good-2025/8428405b8cdd3a3521b61108198113370bff76ef/data\",\n]\n\ndef find_data_dir():\n    for d in CANDIDATE_DATA_DIRS:\n        train_p = os.path.join(d, \"train.json\")\n        test_p  = os.path.join(d, \"test.json\")\n        if os.path.exists(train_p) and os.path.exists(test_p):\n            return d\n    raise FileNotFoundError(\n        \"Could not find train.json/test.json.\\n\"\n        f\"Checked: {CANDIDATE_DATA_DIRS}\\n\"\n        f\"Current working dir: {os.getcwd()}\\n\"\n        f\"Files here: {os.listdir('.')}\"\n    )\n\ndata_dir = find_data_dir()\ntrain_path = os.path.join(data_dir, \"train.json\")\ntest_path  = os.path.join(data_dir, \"test.json\")\n\nprint(\"‚úÖ Using data_dir :\", data_dir)\nprint(\"‚úÖ train_path    :\", train_path)\nprint(\"‚úÖ test_path     :\", test_path)\nprint(\"üìÅ Files in data_dir:\", os.listdir(data_dir))\n\nwith open(train_path, \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\nwith open(test_path, \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\n\ntrain_df = pd.DataFrame(train_data)\ntest_df  = pd.DataFrame(test_data)\n\nprint(\"\\n‚úÖ Train shape:\", train_df.shape)\nprint(\"‚úÖ Test shape :\", test_df.shape)\nprint(\"‚úÖ Train columns:\", train_df.columns.tolist())\nprint(\"‚úÖ Test columns :\", test_df.columns.tolist())\n\n\nassert train_df.shape == (21021, 4), \"Train must be (21021, 4)\"\nassert test_df.shape[0] == 2000, \"Test must have 2000 rows\"\nassert {\"question\", \"context\", \"answer\", \"type\"}.issubset(train_df.columns), \"Train missing required cols\"\nassert {\"ID\", \"question\", \"context\", \"answer\"}.issubset(test_df.columns), \"Test missing required cols\"\nprint(\"‚úÖ Real competition train/test loaded correctly.\")\n\n# =========================\n# 1) Train distribution \n# =========================\nprint(\"\\nüìä Train class counts:\")\nprint(train_df[\"type\"].value_counts())\n\nprint(\"\\nüìä Train class proportions:\")\nprint(train_df[\"type\"].value_counts(normalize=True))\n\n# =========================\n# 2) Helpers (tokens, overlap, negation, numbers/years)\n# =========================\nTOKEN_RE = re.compile(r\"[a-z0-9]+\")\nNUM_RE   = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\b\")\nYEAR_RE  = re.compile(r\"\\b(1[6-9]\\d{2}|20\\d{2}|21\\d{2})\\b\")\n\nNEGATIONS = {\n    \"no\", \"not\", \"never\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\",\n    \"cannot\", \"can't\", \"dont\", \"don't\", \"doesnt\", \"doesn't\", \"didnt\", \"didn't\",\n    \"isnt\", \"isn't\", \"arent\", \"aren't\", \"wasnt\", \"wasn't\", \"werent\", \"weren't\",\n    \"won't\", \"wouldn't\", \"shouldn't\", \"couldn't\", \"mustn't\"\n}\n\ndef safe_str(x):\n    return \"\" if x is None else str(x)\n\ndef normalize_text(s):\n    s = safe_str(s).lower()\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef tokens(s):\n    return TOKEN_RE.findall(normalize_text(s))\n\ndef jaccard(set_a, set_b):\n    if not set_a and not set_b:\n        return 0.0\n    inter = len(set_a & set_b)\n    union = len(set_a | set_b)\n    return inter / union if union else 0.0\n\ndef overlap_ratio(ans_set, other_set):\n    if not ans_set:\n        return 0.0\n    inter = len(ans_set & other_set)\n    return inter / len(ans_set)\n\ndef negation_stats(text):\n    t = tokens(text)\n    neg_count = sum(1 for w in t if w in NEGATIONS)\n    has_neg = 1 if neg_count > 0 else 0\n    return neg_count, has_neg\n\ndef extract_numbers(text):\n    return set(NUM_RE.findall(normalize_text(text)))\n\ndef extract_years(text):\n    return set(YEAR_RE.findall(normalize_text(text)))\n\n# =========================\n# 3) Linguistic feature builder\n# =========================\ndef build_linguistic_features(df: pd.DataFrame) -> csr_matrix:\n    feats = []\n    for _, row in df.iterrows():\n        q = safe_str(row.get(\"question\", \"\"))\n        c = safe_str(row.get(\"context\", \"\"))\n        a = safe_str(row.get(\"answer\", \"\"))\n\n        q_t = set(tokens(q))\n        c_t = set(tokens(c))\n        a_t = set(tokens(a))\n\n        # Overlap features\n        jac_ac = jaccard(a_t, c_t)\n        jac_aq = jaccard(a_t, q_t)\n        ov_ac = overlap_ratio(a_t, c_t)\n        ov_aq = overlap_ratio(a_t, q_t)\n\n        # Negation features\n        neg_a_count, neg_a_has = negation_stats(a)\n        neg_c_count, neg_c_has = negation_stats(c)\n\n        # Length features\n        a_len = len(normalize_text(a))\n        c_len = len(normalize_text(c))\n        q_len = len(normalize_text(q))\n\n        # Number features\n        a_nums = extract_numbers(a)\n        c_nums = extract_numbers(c)\n        num_jac = jaccard(a_nums, c_nums)\n        num_ov = overlap_ratio(a_nums, c_nums)\n        num_a_count = len(a_nums)\n        num_c_count = len(c_nums)\n        num_extra_in_answer = max(0, num_a_count - len(a_nums & c_nums))\n\n        # Year features\n        a_years = extract_years(a)\n        c_years = extract_years(c)\n        year_jac = jaccard(a_years, c_years)\n        year_ov = overlap_ratio(a_years, c_years)\n        year_a_count = len(a_years)\n        year_c_count = len(c_years)\n        year_extra_in_answer = max(0, year_a_count - len(a_years & c_years))\n\n        feats.append([\n            jac_ac, jac_aq, ov_ac, ov_aq,\n            neg_a_count, neg_a_has, neg_c_count, neg_c_has,\n            a_len, c_len, q_len,\n            num_jac, num_ov, num_a_count, num_c_count, num_extra_in_answer,\n            year_jac, year_ov, year_a_count, year_c_count, year_extra_in_answer\n        ])\n\n    X = np.array(feats, dtype=float)\n\n    # Scale lengths\n    X[:, 8]  /= 1000.0\n    X[:, 9]  /= 1000.0\n    X[:, 10] /= 1000.0\n\n    # Scale count-ish features\n    for idx in [13, 14, 15, 18, 19, 20]:\n        X[:, idx] /= 10.0\n\n    return csr_matrix(X)\n\nlinguistic_transformer = FunctionTransformer(build_linguistic_features, validate=False)\n\n# =========================\n# 4) Feature pipeline: Split TF-IDF Q/C/A + Answer char n-grams + Linguistic\n# =========================\ndef select_question(df): return df[\"question\"].astype(str)\ndef select_context(df):  return df[\"context\"].astype(str)\ndef select_answer(df):   return df[\"answer\"].astype(str)\n\nq_selector = FunctionTransformer(select_question, validate=False)\nc_selector = FunctionTransformer(select_context, validate=False)\na_selector = FunctionTransformer(select_answer, validate=False)\n\ntfidf_q = Pipeline([\n    (\"select_q\", q_selector),\n    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words=\"english\", max_features=200000))\n])\n\ntfidf_c = Pipeline([\n    (\"select_c\", c_selector),\n    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words=\"english\", max_features=200000))\n])\n\ntfidf_a = Pipeline([\n    (\"select_a\", a_selector),\n    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words=\"english\", max_features=200000))\n])\n\ntfidf_char_a = Pipeline([\n    (\"select_a\", a_selector),\n    (\"tfidf\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), min_df=3, max_features=200000))\n])\n\nling_branch = Pipeline([\n    (\"ling\", linguistic_transformer)\n])\n\nfeatures = FeatureUnion([\n    (\"tfidf_question\", tfidf_q),\n    (\"tfidf_context\", tfidf_c),\n    (\"tfidf_answer\", tfidf_a),\n    (\"tfidf_answer_char\", tfidf_char_a),\n    (\"linguistic_features\", ling_branch)\n])\n\n# =========================\n# 5) Final Model: Linear SVM\n# =========================\nfinal_model = Pipeline([\n    (\"features\", features),\n    (\"clf\", LinearSVC(class_weight=\"balanced\", C=0.5))\n])\n\n# =========================\n# 6) Validation \n# =========================\nX_all = train_df[[\"question\", \"context\", \"answer\"]]\ny_all = train_df[\"type\"].astype(str).str.lower()\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n)\n\nfinal_model.fit(X_train, y_train)\nval_preds = final_model.predict(X_val)\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"FINAL MODEL: Split TF-IDF (Q/C/A) + Char n-grams + Linguistic + LinearSVC (C=0.5)\")\nprint(\"=\"*90)\nprint(classification_report(y_val, val_preds, digits=3))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_val, val_preds))\nprint(\"Macro F1:\", round(f1_score(y_val, val_preds, average=\"macro\"), 4))\n\n# =========================\n# 7) Train on FULL train + predict test\n# =========================\nfinal_model.fit(X_all, y_all)\ntest_preds = final_model.predict(test_df[[\"question\", \"context\", \"answer\"]])\n\n# =========================\n# 8) submission JSON \n# =========================\nsubmission = [{\"ID\": int(i), \"type\": str(t)} for i, t in zip(test_df[\"ID\"].tolist(), test_preds.tolist())]\n\nallowed = {\"factual\", \"contradiction\", \"irrelevant\"}\nassert len(submission) == 2000\nassert all(row[\"type\"] in allowed for row in submission)\n\nout_path = \"submission_final.json\"\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(submission, f, indent=2)\n\nprint(\"\\n‚úÖ Saved:\", out_path)\nprint(\"‚úÖ Preview:\", submission[:5])\n\n# Optional sanity: predicted distribution\npred_series = pd.Series([r[\"type\"] for r in submission])\nprint(\"\\nüìå Predicted counts:\")\nprint(pred_series.value_counts())\nprint(\"\\nüìå Predicted proportions:\")\nprint(pred_series.value_counts(normalize=True))\n","metadata":{"executionCancelledAt":null,"executionTime":49162,"lastExecutedAt":1767930977599,"lastExecutedByKernel":"be6f455d-352e-4c60-a1e8-8c8d6a4526ae","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# =============================\n# FINAL ALL-IN-ONE PIPELINE (Robust + Correct)\n# - Loads the RIGHT train/test\n# - Prints train distribution\n# - Trains & evaluates (optional but ON by default)\n# - Trains on full train\n# - Writes submission_final.json in required format\n# =============================\n\nimport os, json, re\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom scipy.sparse import csr_matrix\nfrom sklearn.svm import LinearSVC\n\n# =========================\n# 0) Locate correct data files\n# =========================\nCANDIDATE_DATA_DIRS = [\n    \"data\",\n    \"/shared-templates/competition-python-data4good-2025/8428405b8cdd3a3521b61108198113370bff76ef/data\",\n]\n\ndef find_data_dir():\n    for d in CANDIDATE_DATA_DIRS:\n        train_p = os.path.join(d, \"train.json\")\n        test_p  = os.path.join(d, \"test.json\")\n        if os.path.exists(train_p) and os.path.exists(test_p):\n            return d\n    raise FileNotFoundError(\n        \"Could not find train.json/test.json.\\n\"\n        f\"Checked: {CANDIDATE_DATA_DIRS}\\n\"\n        f\"Current working dir: {os.getcwd()}\\n\"\n        f\"Files here: {os.listdir('.')}\"\n    )\n\ndata_dir = find_data_dir()\ntrain_path = os.path.join(data_dir, \"train.json\")\ntest_path  = os.path.join(data_dir, \"test.json\")\n\nprint(\"‚úÖ Using data_dir :\", data_dir)\nprint(\"‚úÖ train_path    :\", train_path)\nprint(\"‚úÖ test_path     :\", test_path)\nprint(\"üìÅ Files in data_dir:\", os.listdir(data_dir))\n\nwith open(train_path, \"r\", encoding=\"utf-8\") as f:\n    train_data = json.load(f)\nwith open(test_path, \"r\", encoding=\"utf-8\") as f:\n    test_data = json.load(f)\n\ntrain_df = pd.DataFrame(train_data)\ntest_df  = pd.DataFrame(test_data)\n\nprint(\"\\n‚úÖ Train shape:\", train_df.shape)\nprint(\"‚úÖ Test shape :\", test_df.shape)\nprint(\"‚úÖ Train columns:\", train_df.columns.tolist())\nprint(\"‚úÖ Test columns :\", test_df.columns.tolist())\n\n# Hard checks so you never accidentally load random test.json again\nassert train_df.shape == (21021, 4), \"Train must be (21021, 4)\"\nassert test_df.shape[0] == 2000, \"Test must have 2000 rows\"\nassert {\"question\", \"context\", \"answer\", \"type\"}.issubset(train_df.columns), \"Train missing required cols\"\nassert {\"ID\", \"question\", \"context\", \"answer\"}.issubset(test_df.columns), \"Test missing required cols\"\nprint(\"‚úÖ Real competition train/test loaded correctly.\")\n\n# =========================\n# 1) Train distribution (THIS answers your question)\n# =========================\nprint(\"\\nüìä Train class counts:\")\nprint(train_df[\"type\"].value_counts())\n\nprint(\"\\nüìä Train class proportions:\")\nprint(train_df[\"type\"].value_counts(normalize=True))\n\n# =========================\n# 2) Helpers (tokens, overlap, negation, numbers/years)\n# =========================\nTOKEN_RE = re.compile(r\"[a-z0-9]+\")\nNUM_RE   = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\b\")\nYEAR_RE  = re.compile(r\"\\b(1[6-9]\\d{2}|20\\d{2}|21\\d{2})\\b\")\n\nNEGATIONS = {\n    \"no\", \"not\", \"never\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\",\n    \"cannot\", \"can't\", \"dont\", \"don't\", \"doesnt\", \"doesn't\", \"didnt\", \"didn't\",\n    \"isnt\", \"isn't\", \"arent\", \"aren't\", \"wasnt\", \"wasn't\", \"werent\", \"weren't\",\n    \"won't\", \"wouldn't\", \"shouldn't\", \"couldn't\", \"mustn't\"\n}\n\ndef safe_str(x):\n    return \"\" if x is None else str(x)\n\ndef normalize_text(s):\n    s = safe_str(s).lower()\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef tokens(s):\n    return TOKEN_RE.findall(normalize_text(s))\n\ndef jaccard(set_a, set_b):\n    if not set_a and not set_b:\n        return 0.0\n    inter = len(set_a & set_b)\n    union = len(set_a | set_b)\n    return inter / union if union else 0.0\n\ndef overlap_ratio(ans_set, other_set):\n    if not ans_set:\n        return 0.0\n    inter = len(ans_set & other_set)\n    return inter / len(ans_set)\n\ndef negation_stats(text):\n    t = tokens(text)\n    neg_count = sum(1 for w in t if w in NEGATIONS)\n    has_neg = 1 if neg_count > 0 else 0\n    return neg_count, has_neg\n\ndef extract_numbers(text):\n    return set(NUM_RE.findall(normalize_text(text)))\n\ndef extract_years(text):\n    return set(YEAR_RE.findall(normalize_text(text)))\n\n# =========================\n# 3) Linguistic feature builder\n# =========================\ndef build_linguistic_features(df: pd.DataFrame) -> csr_matrix:\n    feats = []\n    for _, row in df.iterrows():\n        q = safe_str(row.get(\"question\", \"\"))\n        c = safe_str(row.get(\"context\", \"\"))\n        a = safe_str(row.get(\"answer\", \"\"))\n\n        q_t = set(tokens(q))\n        c_t = set(tokens(c))\n        a_t = set(tokens(a))\n\n        # Overlap features\n        jac_ac = jaccard(a_t, c_t)\n        jac_aq = jaccard(a_t, q_t)\n        ov_ac = overlap_ratio(a_t, c_t)\n        ov_aq = overlap_ratio(a_t, q_t)\n\n        # Negation features\n        neg_a_count, neg_a_has = negation_stats(a)\n        neg_c_count, neg_c_has = negation_stats(c)\n\n        # Length features\n        a_len = len(normalize_text(a))\n        c_len = len(normalize_text(c))\n        q_len = len(normalize_text(q))\n\n        # Number features\n        a_nums = extract_numbers(a)\n        c_nums = extract_numbers(c)\n        num_jac = jaccard(a_nums, c_nums)\n        num_ov = overlap_ratio(a_nums, c_nums)\n        num_a_count = len(a_nums)\n        num_c_count = len(c_nums)\n        num_extra_in_answer = max(0, num_a_count - len(a_nums & c_nums))\n\n        # Year features\n        a_years = extract_years(a)\n        c_years = extract_years(c)\n        year_jac = jaccard(a_years, c_years)\n        year_ov = overlap_ratio(a_years, c_years)\n        year_a_count = len(a_years)\n        year_c_count = len(c_years)\n        year_extra_in_answer = max(0, year_a_count - len(a_years & c_years))\n\n        feats.append([\n            jac_ac, jac_aq, ov_ac, ov_aq,\n            neg_a_count, neg_a_has, neg_c_count, neg_c_has,\n            a_len, c_len, q_len,\n            num_jac, num_ov, num_a_count, num_c_count, num_extra_in_answer,\n            year_jac, year_ov, year_a_count, year_c_count, year_extra_in_answer\n        ])\n\n    X = np.array(feats, dtype=float)\n\n    # Scale lengths\n    X[:, 8]  /= 1000.0\n    X[:, 9]  /= 1000.0\n    X[:, 10] /= 1000.0\n\n    # Scale count-ish features\n    for idx in [13, 14, 15, 18, 19, 20]:\n        X[:, idx] /= 10.0\n\n    return csr_matrix(X)\n\nlinguistic_transformer = FunctionTransformer(build_linguistic_features, validate=False)\n\n# =========================\n# 4) Feature pipeline: Split TF-IDF Q/C/A + Answer char n-grams + Linguistic\n# =========================\ndef select_question(df): return df[\"question\"].astype(str)\ndef select_context(df):  return df[\"context\"].astype(str)\ndef select_answer(df):   return df[\"answer\"].astype(str)\n\nq_selector = FunctionTransformer(select_question, validate=False)\nc_selector = FunctionTransformer(select_context, validate=False)\na_selector = FunctionTransformer(select_answer, validate=False)\n\ntfidf_q = Pipeline([\n    (\"select_q\", q_selector),\n    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words=\"english\", max_features=200000))\n])\n\ntfidf_c = Pipeline([\n    (\"select_c\", c_selector),\n    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words=\"english\", max_features=200000))\n])\n\ntfidf_a = Pipeline([\n    (\"select_a\", a_selector),\n    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words=\"english\", max_features=200000))\n])\n\ntfidf_char_a = Pipeline([\n    (\"select_a\", a_selector),\n    (\"tfidf\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), min_df=3, max_features=200000))\n])\n\nling_branch = Pipeline([\n    (\"ling\", linguistic_transformer)\n])\n\nfeatures = FeatureUnion([\n    (\"tfidf_question\", tfidf_q),\n    (\"tfidf_context\", tfidf_c),\n    (\"tfidf_answer\", tfidf_a),\n    (\"tfidf_answer_char\", tfidf_char_a),\n    (\"linguistic_features\", ling_branch)\n])\n\n# =========================\n# 5) Final Model: Linear SVM\n# =========================\nfinal_model = Pipeline([\n    (\"features\", features),\n    (\"clf\", LinearSVC(class_weight=\"balanced\", C=0.5))\n])\n\n# =========================\n# 6) Validation (keep ON for report)\n# =========================\nX_all = train_df[[\"question\", \"context\", \"answer\"]]\ny_all = train_df[\"type\"].astype(str).str.lower()\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n)\n\nfinal_model.fit(X_train, y_train)\nval_preds = final_model.predict(X_val)\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"FINAL MODEL: Split TF-IDF (Q/C/A) + Char n-grams + Linguistic + LinearSVC (C=0.5)\")\nprint(\"=\"*90)\nprint(classification_report(y_val, val_preds, digits=3))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_val, val_preds))\nprint(\"Macro F1:\", round(f1_score(y_val, val_preds, average=\"macro\"), 4))\n\n# =========================\n# 7) Train on FULL train + predict test\n# =========================\nfinal_model.fit(X_all, y_all)\ntest_preds = final_model.predict(test_df[[\"question\", \"context\", \"answer\"]])\n\n# =========================\n# 8) Save submission JSON (required format)\n# =========================\nsubmission = [{\"ID\": int(i), \"type\": str(t)} for i, t in zip(test_df[\"ID\"].tolist(), test_preds.tolist())]\n\nallowed = {\"factual\", \"contradiction\", \"irrelevant\"}\nassert len(submission) == 2000\nassert all(row[\"type\"] in allowed for row in submission)\n\nout_path = \"submission_final.json\"\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(submission, f, indent=2)\n\nprint(\"\\n‚úÖ Saved:\", out_path)\nprint(\"‚úÖ Preview:\", submission[:5])\n\n# Optional sanity: predicted distribution\npred_series = pd.Series([r[\"type\"] for r in submission])\nprint(\"\\nüìå Predicted counts:\")\nprint(pred_series.value_counts())\nprint(\"\\nüìå Predicted proportions:\")\nprint(pred_series.value_counts(normalize=True))\n","outputsMetadata":{"0":{"height":517,"type":"stream"},"1":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"39db2878-6097-4542-a233-bfeafa77a40d","nodeType":"const"}}},"2":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"055159cb-bbd3-4439-947a-30c4cf5ec12a","nodeType":"const"}}},"3":{"height":59,"type":"stream"},"4":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"055159cb-bbd3-4439-947a-30c4cf5ec12a","nodeType":"const"}}},"5":{"height":59,"type":"stream"},"6":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"055159cb-bbd3-4439-947a-30c4cf5ec12a","nodeType":"const"}}},"7":{"height":517,"type":"stream"},"8":{"height":59,"type":"stream"},"9":{"height":521,"type":"stream"},"13":{"height":59,"type":"stream"},"14":{"height":395,"type":"stream"},"16":{"height":80,"type":"stream"},"18":{"height":59,"type":"stream"},"20":{"height":269,"type":"stream"}}},"id":"cfe5bbae-0676-4b66-ac9e-1e2fe5427bac","cell_type":"code","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"‚úÖ Using data_dir : data\n‚úÖ train_path    : data/train.json\n‚úÖ test_path     : data/test.json\nüìÅ Files in data_dir: ['test.json', 'train.json']\n\n‚úÖ Train shape: (21021, 4)\n‚úÖ Test shape : (2000, 5)\n‚úÖ Train columns: ['answer', 'type', 'context', 'question']\n‚úÖ Test columns : ['ID', 'answer', 'type', 'context', 'question']\n‚úÖ Real competition train/test loaded correctly.\n\nüìä Train class counts:\ntype\nfactual          17431\ncontradiction     1818\nirrelevant        1772\nName: count, dtype: int64\n\nüìä Train class proportions:\ntype\nfactual          0.829218\ncontradiction    0.086485\nirrelevant       0.084297\nName: proportion, dtype: float64\n\n==========================================================================================\nFINAL MODEL: Split TF-IDF (Q/C/A) + Char n-grams + Linguistic + LinearSVC (C=0.5)\n==========================================================================================\n               precision    recall  f1-score   support\n\ncontradiction      0.834     0.635     0.721       364\n      factual      0.962     0.987     0.974      3487\n   irrelevant      0.903     0.898     0.901       354\n\n     accuracy                          0.949      4205\n    macro avg      0.900     0.840     0.865      4205\n weighted avg      0.946     0.949     0.946      4205\n\nConfusion Matrix:\n [[ 231  113   20]\n [  33 3440   14]\n [  13   23  318]]\nMacro F1: 0.8652\n\n‚úÖ Saved: submission_final.json\n‚úÖ Preview: [{'ID': 1, 'type': 'factual'}, {'ID': 2, 'type': 'factual'}, {'ID': 3, 'type': 'factual'}, {'ID': 4, 'type': 'factual'}, {'ID': 5, 'type': 'factual'}]\n\nüìå Predicted counts:\nfactual          1710\nirrelevant        181\ncontradiction     109\nName: count, dtype: int64\n\nüìå Predicted proportions:\nfactual          0.8550\nirrelevant       0.0905\ncontradiction    0.0545\nName: proportion, dtype: float64\n"}]},{"source":"## üí™ Competition challenge\n\nCreate a report here in DataLab that covers the following:\n1. Your EDA and machine learning process using the `data/train.json` file.\n2. Complete the `data/test.json` file by predicting the `type` of answer for each question (2000 total). The `data/test.json` file also has an `ID` column, which uniquely identifies each row.\n","metadata":{},"id":"0c812779-db6a-47b5-9927-fe9a9544d68d","cell_type":"markdown"},{"source":"## ‚úÖ Submission Instructions:\n\n* First, submit your DataLab workbook using the button in the top right corner.\n* Then, submit your predictions as a **.json file** via [this form](https://forms.gle/hKFHqPkaeQkfAx4v7).\n* The structure of the file should not be altered, and should include the IDs and predicted answer `type`.\n*  Only one team member needs to submit. List the **academic** emails of **all** the team members in the submission form.\n\n\n","metadata":{},"id":"c6a21a6b-7694-401d-a460-ae5fcb73c462","cell_type":"markdown"},{"source":"## üßë‚Äç‚öñÔ∏è Judging criteria\n\nYour submission will be scored using a custom weighted confusion matrix to account for cost-based priorities. Each class receives an equal weighting to calculate your overall prediction performance. Thus, factual prediction score counts for 33.3%, contradiction classification for 33.3%, and irrelevant for 33.3%.\n\nYour classification evaluation on the test set will be ranked among all teams in the competition. A total of 6000 points (34.60% of the total amount of points of the Data4Good challenge) can be earned by successfully submitting the competition.","metadata":{},"id":"30373ef9-6fc3-434c-b9e0-18d482f0dd81","cell_type":"markdown"},{"source":"## ‚úÖ Checklist before publishing into the competition\n- Rename your workspace to make it descriptive of your work. N.B. you should leave the notebook name as notebook.ipynb.\n- **Remove redundant cells** like the judging criteria, so the workbook is focused on your story.\n- Make sure the workbook reads well and explains how you found your insights. \n- Try to include an **executive summary** of your recommendations at the beginning.\n- Check that all the cells run without error.","metadata":{},"id":"49e09c46-c39d-488a-b025-cb25e5605c1c","cell_type":"markdown"},{"source":"## ‚åõÔ∏è Time is ticking. Good luck!","metadata":{},"id":"c236ec42-9d87-4526-8389-ffa9427ef796","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (User venv)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}